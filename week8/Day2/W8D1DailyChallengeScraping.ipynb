{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMN5HdwiLAIxxgBE1KwumCl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silvia-denanni/DI-Bootcamp-nov25/blob/main/W8D1DailyChallengeScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dlhEtaU_-eh"
      },
      "outputs": [],
      "source": [
        "#-----------------------------1) Write a Python script using the requests library to fetch the HTML\n",
        "# content of the chosen website.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# url = 'https://github.com/topics/awesome'\n",
        "# headers = {\n",
        "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "# }\n",
        "\n",
        "# try:\n",
        "#     print(\"Fetching page...\")\n",
        "#     time.sleep(1)  # Be polite\n",
        "#     response = requests.get(url, headers=headers, timeout=(10, 30))\n",
        "#     response.raise_for_status()  # Raises error for bad status codes\n",
        "\n",
        "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
        "#     print(\"Success! Page title:\", soup.title.text if soup.title else \"No title\")\n",
        "#     print(soup.prettify()[:1000])  # Pretty formatted HTML\n",
        "\n",
        "# except requests.exceptions.Timeout:\n",
        "#     print(\"Timeout - server too slow. Try later.\")\n",
        "# except requests.exceptions.ConnectionError:\n",
        "#     print(\"Connection failed - check internet.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error: {e}\")                           #Fetching page...Timeout - server too slow. Try later.\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------2)Print the status code of the response to ensure the request\n",
        "#was successful using .status_code, it should be 200.\n",
        "# url = 'https://github.com/topics/awesome'\n",
        "# headers = {\n",
        "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "# }\n",
        "# response = requests.get(url, headers=headers)\n",
        "# print(f\"Status: {response.status_code}\")\n",
        "\n",
        "\n",
        "#-----------------------------3)Print the first 100 characters of the HTML content to verify the response.\n",
        "# url = 'https://github.com/topics/awesome'\n",
        "# headers = {\n",
        "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "# }\n",
        "# response = requests.get(url, headers=headers, timeout=(10, 30))\n",
        "# soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# print(soup.prettify()[:100])  # Pretty formatted HTML\n",
        "\n",
        "\n",
        "#-----------------------------4)Save the HTML content to a file named webpage.html.\n",
        "# Ensure you handle the text encoding correctly.\n",
        "# url = 'https://github.com/topics/awesome'\n",
        "# headers = {\n",
        "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "# }\n",
        "# response = requests.get(url, headers=headers)\n",
        "# response.raise_for_status()  # Raises error for bad status\n",
        "\n",
        "# # Save HTML content to file with UTF-8 encoding\n",
        "# with open('webpage.html', 'w', encoding='utf-8') as file:   ##'with'  magic (automatic cleanup): After I'm done writing, automatically close the file so nothing gets corrupted.\n",
        "\n",
        "#     file.write(response.text)\n",
        "\n",
        "# # \"Open a new file called 'webpage.html'\n",
        "# #in WRITE mode ('w')\n",
        "# #using UTF-8 language (encoding='utf-8')\n",
        "# #and give me a handle called 'file'\"\n",
        "\n",
        "# print(\"HTML content saved to 'webpage.html' successfully!\")\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------5)Use BeautifulSoup to parse the saved HTML content.\n",
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# url = 'https://github.com/topics/awesome'\n",
        "# headers = {\n",
        "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "# }\n",
        "\n",
        "# # Fetch the page with timeout\n",
        "# response = requests.get(url, headers=headers, timeout=10)\n",
        "# response.raise_for_status()  # Raises error for bad status\n",
        "\n",
        "# # Save HTML content to file with UTF-8 encoding\n",
        "# with open('webpage.html', 'w', encoding='utf-8') as file:\n",
        "#     file.write(response.text)\n",
        "\n",
        "# print(\"HTML content saved to 'webpage.html' successfully!\")\n",
        "\n",
        "# #Parse and preview\n",
        "# soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# with open('webpage.html', 'r', encoding='utf-8') as file:\n",
        "#     print(file.read()[:500])\n",
        "\n",
        "\n",
        "#-----------------------------6)Identify two distinct pieces of information on the webpage to extract\n",
        "# (e.g., titles of the topics and their descriptions).\n",
        "#-----------------------------7)Write code to extract these pieces of information. Ensure you identify\n",
        "# the correct HTML tags and classes used for these elements on the webpage.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Load from saved file\n",
        "with open('webpage.html', 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "print(\"Awesome Lists with Descriptions:\\n\")\n",
        "\n",
        "# Find all h3 titles AND their matching descriptions\n",
        "h3_items = soup.find_all('h3')\n",
        "\n",
        "for i, h3 in enumerate(h3_items, 1):\n",
        "    title = h3.get_text(strip=True)\n",
        "\n",
        "    # Look for the SPECIFIC description paragraph after this h3\n",
        "    description_elem = h3.find_next('p', class_='color-fg-muted mb-0')\n",
        "\n",
        "    if description_elem:\n",
        "        description = description_elem.get_text(strip=True)\n",
        "        print(f\"{i}. **{title}**\")\n",
        "        print(f\"   {description}\\n\")\n",
        "    else:\n",
        "        print(f\"{i}. **{title}**\")\n",
        "        print(\"   No description found\\n\")\n",
        "\n",
        "\n",
        "#-----------------------------8)Create a Python dictionary to structure the extracted data, with keys\n",
        "# representing the type of information (e.g., ‘title’ and ‘description’).\n",
        "#-----------------------------9)Convert this dictionary into a pandas DataFrame.\n",
        "#-----------------------------10)Print the DataFrame to confirm its structure and contents.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Load from saved file\n",
        "with open('webpage.html', 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Step 1: Create dictionary to store data\n",
        "data = []\n",
        "\n",
        "# Step 2: Find all h3 titles and their descriptions\n",
        "h3_items = soup.find_all('h3')\n",
        "\n",
        "for h3 in h3_items:\n",
        "    title = h3.get_text(strip=True)\n",
        "\n",
        "    # Find matching description\n",
        "    description_elem = h3.find_next('p', class_='color-fg-muted mb-0')\n",
        "    description = description_elem.get_text(strip=True) if description_elem else \"No description\"\n",
        "\n",
        "    # Step 3: Add to dictionary as a record\n",
        "    data.append({\n",
        "        'title': title,\n",
        "        'description': description\n",
        "    })\n",
        "\n",
        "# Step 4: Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 5: Print DataFrame\n",
        "print(\"Awesome Lists DataFrame:\")\n",
        "print(df)\n",
        "\n",
        "print(f\"\\nShape: {df.shape}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())"
      ]
    }
  ]
}